{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-R1:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-R1:\n",
      "- modeling_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-ai/DeepSeek-R1\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    558\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m add_generation_mixin_to_remote_model(model_class)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    561\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/modeling_utils.py:3442\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[0;32m-> 3442\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m AutoHfQuantizer\u001b[38;5;241m.\u001b[39mmerge_quantization_configs(\n\u001b[1;32m   3443\u001b[0m             config\u001b[38;5;241m.\u001b[39mquantization_config, quantization_config\n\u001b[1;32m   3444\u001b[0m         )\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3446\u001b[0m         config\u001b[38;5;241m.\u001b[39mquantization_config \u001b[38;5;241m=\u001b[39m quantization_config\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/quantizers/auto.py:169\u001b[0m, in \u001b[0;36mAutoHfQuantizer.merge_quantization_configs\u001b[0;34m(cls, quantization_config, quantization_config_from_args)\u001b[0m\n\u001b[1;32m    166\u001b[0m     warning_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantization_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 169\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m AutoQuantizationConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config))\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m quantization_config_from_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    174\u001b[0m ):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     loading_attr_dict \u001b[38;5;241m=\u001b[39m quantization_config_from_args\u001b[38;5;241m.\u001b[39mget_loading_attributes()\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface/lib/python3.11/site-packages/transformers/quantizers/auto.py:93\u001b[0m, in \u001b[0;36mAutoQuantizationConfig.from_dict\u001b[0;34m(cls, quantization_config_dict)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quant_method \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m AUTO_QUANTIZATION_CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown quantization type, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - supported types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(AUTO_QUANTIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[1;32m     98\u001b[0m target_cls \u001b[38;5;241m=\u001b[39m AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target_cls\u001b[38;5;241m.\u001b[39mfrom_dict(quantization_config_dict)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown quantization type, got fp8 - supported types are: ['awq', 'bitsandbytes_4bit', 'bitsandbytes_8bit', 'gptq', 'aqlm', 'quanto', 'eetq', 'hqq', 'compressed-tensors', 'fbgemm_fp8', 'torchao']"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hfsg/miniconda3/envs/huggingface/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151646,  12689,    752,   3270,    264,   4771,    821,   5944,   8129,\n",
       "            304,  10135]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Qwen2Tokenizer, Qwen2ForCausalLM\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "input_text = \"Help me write a graph data structure implementation in python\"  # Replace with your prompt\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    # output = model(**inputs)\n",
    "    outputs = model.generate(**inputs, max_length=1000, do_sample=True, temperature=0.7)\n",
    "\n",
    "# generated_text = tokenizer.decode(output.logits[0].argmax(dim=-1))\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Help me write a graph data structure implementation in python.\n",
      "Okay, I need to write a graph data structure in Python, and I need to make sure it's efficient and easy to use. I remember that a graph consists of nodes (also called vertices) and edges, which connect these nodes. Each edge can be either directed or undirected. For this project, I think I should focus on undirected graphs because I'm more comfortable with that. \n",
      "\n",
      "Wait, but how do I represent an undirected graph in Python? I've heard of adjacency lists, where each node has a list of its connected nodes. That sounds manageable. So, maybe I can create a class for the graph that has a dictionary where each key is a node, and the value is the list of connected nodes. \n",
      "\n",
      "But before I jump into coding, I should think about the structure of the graph. Should I include a method to add nodes and edges? I think yes, because users will need to create and modify the graph as they go. Also, I should consider the possibility of multiple edges between two nodes or loops (edges that connect a node to itself). Hmm, but for simplicity, maybe I should allow multiple edges but not loops. Or perhaps I can handle loops by allowing a node to be connected to itself.\n",
      "\n",
      "Wait, the problem statement doesn't specify any constraints on the graph, so perhaps it's better to handle all possible cases. So, in my implementation, I should allow for each node to have multiple edges to other nodes and also allow a node to have a self-loop.\n",
      "\n",
      "Okay, so first, I'll create a class called Graph. This class will have an adjacency dictionary, which is a mapping from nodes to their connected nodes. The initial graph will be empty. \n",
      "\n",
      "Next, I need to think about methods to add nodes and edges. For adding nodes, I can have a method called add_node, which takes a node name as an argument and adds it to the adjacency dictionary. For adding edges, since it's an undirected graph, when I add an edge between node A and node B, I need to add B to A's adjacency list and A to B's adjacency list. But I also need to ensure that the edge is added only once. So, perhaps I should check if B is already in A's adjacency list before adding it.\n",
      "\n",
      "Wait, but in an undirected graph, if I add an edge between A and B, it's the same as adding an edge between B and A. So, to avoid adding duplicates, I can check if B is in A's adjacency list. If not, add it. Otherwise, don't add it again. So, that way, each edge is added only once.\n",
      "\n",
      "But what about the order of adding edges? Since the graph is undirected, adding an edge A-B is the same as adding B-A. So, perhaps the add_edge method can take two nodes and add both directions. But wait, in an undirected graph, when adding an edge A-B, both A's adjacency list and B's adjacency list should include each other. So, the add_edge method should add both B to A's list and A to B's list.\n",
      "\n",
      "But wait, if the graph is undirected, then adding an edge A-B is the same as adding it B-A. So, perhaps the add_edge method should be used to add both directions. But in that case, when adding edges, I need to make sure that I don't add duplicates.\n",
      "\n",
      "Alternatively, the add_node method can be used to add nodes, and the add_edge method can be used to add edges between existing nodes. But that might complicate things because the add_edge method could be called with the same nodes twice, which would cause issues.\n",
      "\n",
      "Hmm, perhaps it's better to have an add_node method that adds nodes and an add_edge method that adds edges between nodes. But then, the add_edge method would need to check if the nodes already exist in the graph and whether they are connected. Wait, no, because add_edge could be called with any two nodes, and we need to make sure that adding an edge between them is allowed.\n",
      "\n",
      "Alternatively, maybe the add_edge method can take two nodes and add both directions if they are not already present or if they are already present but the edge doesn't exist.\n",
      "\n",
      "Wait, perhaps the add_edge method can be used to add an edge between two nodes that may already exist in the graph. So, when adding an edge between A and B, if A is already in the graph, we check if B is already in A's adjacency list. If not, add it. Similarly, if B is already in the graph, check if A is in B's adjacency list. If not, add it. Otherwise, don't add it.\n",
      "\n",
      "But in an undirected graph, adding an edge between A and B should be the same as adding an edge between B and A\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
